"""
–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ ‚Ññ1
–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º pandas
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler

def load_data(file_path):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ CSV —Ñ–∞–π–ª–∞"""
    return pd.read_csv(file_path)

def display_basic_info(df):
    """–í—ã–≤–æ–¥ –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–∞—Ç–∞—Å–µ—Ç–µ"""
    print("=== –ë–ê–ó–û–í–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –î–ê–¢–ê–°–ï–¢–ï ===")
    print(f"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫: {df.shape[0]}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–æ–ª–±—Ü–æ–≤: {df.shape[1]}")

    print("\n–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ –¥–∞—Ç–∞—Å–µ—Ç–∞:")
    print(df.head())

    print("\n–ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 —Å—Ç—Ä–æ–∫ –¥–∞—Ç–∞—Å–µ—Ç–∞:")
    print(df.tail())

    print("\n–ù–∞–∑–≤–∞–Ω–∏—è —Å—Ç–æ–ª–±—Ü–æ–≤:")
    print(df.columns.tolist())

    print("\n–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:")
    print(df.dtypes)

    print("\n–ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤:")
    print(df.describe())

    print("\n–ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤:")
    categorical_cols = df.select_dtypes(include=['object']).columns
    if len(categorical_cols) > 0:
        print(df[categorical_cols].describe())

def visualize_missing_values(df):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
    plt.figure(figsize=(12, 6))

    # Heatmap –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    plt.subplot(1, 2, 1)
    sns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='viridis')
    plt.title('Heatmap –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π')

    # Bar plot –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ —Å—Ç–æ–ª–±—Ü–∞–º
    plt.subplot(1, 2, 2)
    missing_counts = df.isnull().sum()
    missing_percent = (df.isnull().sum() / len(df)) * 100
    missing_df = pd.DataFrame({
        '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤': missing_counts,
        '–ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤': missing_percent
    }).sort_values('–ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤', ascending=False)

    missing_df[missing_df['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤'] > 0]['–ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤'].plot(kind='bar')
    plt.title('–ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤ –ø–æ —Å—Ç–æ–ª–±—Ü–∞–º')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

def analyze_missing_values(df):
    """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
    print("=== –ê–ù–ê–õ–ò–ó –ü–†–û–ü–£–©–ï–ù–ù–´–• –ó–ù–ê–ß–ï–ù–ò–ô ===")
    missing_info = pd.DataFrame({
        '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤': df.isnull().sum(),
        '–ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤': (df.isnull().sum() / len(df)) * 100
    }).sort_values('–ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤', ascending=False)

    # –í—ã–≤–æ–¥–∏–º —Ç–æ–ª—å–∫–æ —Å—Ç–æ–ª–±—Ü—ã —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏
    missing_columns = missing_info[missing_info['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤'] > 0]
    if len(missing_columns) > 0:
        print("–°—Ç–æ–ª–±—Ü—ã —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏:")
        print(missing_columns)
    else:
        print("–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ!")

    print(f"\n–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ: {df.isnull().sum().sum()}")

    return missing_info

def fill_missing_values(df):
    """–ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
    print("=== –ó–ê–ü–û–õ–ù–ï–ù–ò–ï –ü–†–û–ü–£–©–ï–ù–ù–´–• –ó–ù–ê–ß–ï–ù–ò–ô ===")
    df_filled = df.copy()

    # –ê–Ω–∞–ª–∏–∑ –¥–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è
    missing_before = df.isnull().sum()
    columns_with_missing = missing_before[missing_before > 0].index.tolist()

    if not columns_with_missing:
        print("–ù–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è.")
        return df_filled

    print("–°—Ç–æ–ª–±—Ü—ã —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏:")
    for col in columns_with_missing:
        print(f"  - {col}: {missing_before[col]} –ø—Ä–æ–ø—É—Å–∫–æ–≤")

    # –ß–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã
    numeric_columns = df_filled.select_dtypes(include=['number']).columns
    numeric_with_missing = [col for col in numeric_columns if col in columns_with_missing]

    for col in numeric_with_missing:
        if df_filled[col].isnull().sum() > 0:
            median_val = df_filled[col].median()
            df_filled[col].fillna(median_val, inplace=True)
            print(f"–ó–∞–ø–æ–ª–Ω–µ–Ω —á–∏—Å–ª–æ–≤–æ–π —Å—Ç–æ–ª–±–µ—Ü '{col}' –º–µ–¥–∏–∞–Ω–æ–π: {median_val:.2f}")

    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã
    categorical_columns = df_filled.select_dtypes(include=['object']).columns
    categorical_with_missing = [col for col in categorical_columns if col in columns_with_missing]

    for col in categorical_with_missing:
        if df_filled[col].isnull().sum() > 0:
            mode_value = df_filled[col].mode()[0] if not df_filled[col].mode().empty else 'Unknown'
            df_filled[col].fillna(mode_value, inplace=True)
            print(f"–ó–∞–ø–æ–ª–Ω–µ–Ω –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π —Å—Ç–æ–ª–±–µ—Ü '{col}' –º–æ–¥–æ–π: '{mode_value}'")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Å–ª–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è
    missing_after = df_filled.isnull().sum().sum()
    print(f"\n–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Å–ª–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è:")
    print(f"–û—Å—Ç–∞–ª–æ—Å—å –ø—Ä–æ–ø—É—Å–∫–æ–≤: {missing_after}")

    return df_filled

def normalize_data(df):
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
    print("=== –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –î–ê–ù–ù–´–• ===")
    df_normalized = df.copy()

    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
    numeric_columns = df_normalized.select_dtypes(include=['number']).columns.tolist()

    # –ò—Å–∫–ª—é—á–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
    exclude_columns = ['Survived', 'target']  # –î–æ–±–∞–≤—å—Ç–µ –¥—Ä—É–≥–∏–µ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
    columns_to_normalize = [col for col in numeric_columns if col not in exclude_columns]

    if not columns_to_normalize:
        print("–ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.")
        return df_normalized

    print(f"–ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º—ã–µ —Å—Ç–æ–ª–±—Ü—ã: {columns_to_normalize}")

    # MinMaxScaler
    minmax_scaler = MinMaxScaler()
    minmax_data = minmax_scaler.fit_transform(df_normalized[columns_to_normalize])
    df_minmax = pd.DataFrame(minmax_data,
                           columns=[f'{col}_minmax' for col in columns_to_normalize],
                           index=df_normalized.index)

    # StandardScaler
    standard_scaler = StandardScaler()
    standard_data = standard_scaler.fit_transform(df_normalized[columns_to_normalize])
    df_standard = pd.DataFrame(standard_data,
                             columns=[f'{col}_standard' for col in columns_to_normalize],
                             index=df_normalized.index)

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—ã
    df_normalized = pd.concat([df_normalized, df_minmax, df_standard], axis=1)

    print(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {len(columns_to_normalize) * 2} –Ω–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏")
    print(f"–†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {df_normalized.shape}")

    return df_normalized

def encode_categorical_data(df):
    """One-Hot Encoding –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    print("=== –ö–û–î–ò–†–û–í–ê–ù–ò–ï –ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–´–• –î–ê–ù–ù–´–• ===")

    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
    categorical_columns = df.select_dtypes(include=['object']).columns.tolist()

    if not categorical_columns:
        print("–ù–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è.")
        return df

    print(f"–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è: {categorical_columns}")

    # –ü—Ä–∏–º–µ–Ω—è–µ–º One-Hot Encoding
    df_encoded = pd.get_dummies(df, columns=categorical_columns, drop_first=True, prefix_sep='_')

    print(f"–†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è: {df_encoded.shape}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è: {len(df.columns)}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–æ–ª–±—Ü–æ–≤ –ø–æ—Å–ª–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è: {len(df_encoded.columns)}")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–æ–≤—ã–µ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã
    new_columns = set(df_encoded.columns) - set(df.columns)
    print(f"–°–æ–∑–¥–∞–Ω–æ {len(new_columns)} –Ω–æ–≤—ã—Ö –±–∏–Ω–∞—Ä–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤")

    return df_encoded

def create_requirements_file():
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ requirements.txt"""
    requirements = """pandas>=2.0.0
numpy>=1.24.0
scikit-learn>=1.3.0
matplotlib>=3.7.0
seaborn>=0.12.0
"""
    with open('requirements.txt', 'w', encoding='utf-8') as f:
        f.write(requirements)
    print("–°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª requirements.txt")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        print("=" * 60)
        print("–õ–ê–ë–û–†–ê–¢–û–†–ù–ê–Ø –†–ê–ë–û–¢–ê ‚Ññ1: –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–•")
        print("=" * 60)

        print("\n–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        df = load_data('titanic.csv')

        # –®–∞–≥ 1: –í—ã–≤–æ–¥ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —ç–∫—Ä–∞–Ω
        display_basic_info(df)

        # –®–∞–≥ 2: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        visualize_missing_values(df)

        # –®–∞–≥ 3: –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        missing_info = analyze_missing_values(df)

        # –®–∞–≥ 4: –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        df_filled = fill_missing_values(df)

        # –®–∞–≥ 5: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        df_normalized = normalize_data(df_filled)

        # –®–∞–≥ 6: –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        df_encoded = encode_categorical_data(df_normalized)

        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        print("\n=== –§–ò–ù–ê–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê ===")

        # –£–¥–∞–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã –∏ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω—ã
        numeric_columns = df_filled.select_dtypes(include=['number']).columns
        categorical_columns = df_filled.select_dtypes(include=['object']).columns

        # –ò—Å–∫–ª—é—á–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ —É–¥–∞–ª–µ–Ω–∏—è
        exclude_from_drop = ['Survived', 'target']
        columns_to_drop = [col for col in numeric_columns if col not in exclude_from_drop] + categorical_columns.tolist()

        df_final = df_encoded.drop(columns=columns_to_drop, errors='ignore')
        print(f"–£–¥–∞–ª–µ–Ω–æ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤: {len(columns_to_drop)}")
        print(f"–§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df_final.shape}")

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
        print("\n=== –†–ê–ó–î–ï–õ–ï–ù–ò–ï –ù–ê TRAIN/TEST ===")
        train_df, test_df = train_test_split(df_final, test_size=0.3, random_state=42, stratify=df_final.get('Survived', None))
        print(f"Train set: {train_df.shape}")
        print(f"Test set: {test_df.shape}")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("\n=== –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===")
        train_df.to_csv('processed_titanic_train.csv', index=False)
        test_df.to_csv('processed_titanic_test.csv', index=False)

        # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ requirements.txt
        create_requirements_file()

        # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
        print("\n" + "=" * 60)
        print("–ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
        print("=" * 60)
        print(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {df.shape}")
        print(f"–§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {df_final.shape}")
        print(f"Train set: {train_df.shape}")
        print(f"Test set: {test_df.shape}")
        print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
        print("  - processed_titanic_train.csv")
        print("  - processed_titanic_test.csv")
        print("  - requirements.txt")

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–¥–µ–ª–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ
        print("\n–í–´–ü–û–õ–ù–ï–ù–ù–´–ï –≠–¢–ê–ü–´ –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ò:")
        print("‚úÖ 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã–≤–æ–¥ –Ω–∞ —ç–∫—Ä–∞–Ω")
        print("‚úÖ 2. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")
        print("‚úÖ 3. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")
        print("‚úÖ 4. –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (–º–µ–¥–∏–∞–Ω–∞/–º–æ–¥–∞)")
        print("‚úÖ 5. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (MinMaxScaler + StandardScaler)")
        print("‚úÖ 6. One-Hot Encoding –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        print("‚úÖ 7. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏")
        print("‚úÖ 8. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")

        print("\nüéâ –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–• –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
        print("=" * 60)

    except FileNotFoundError:
        print("‚ùå –û—à–∏–±–∫–∞: –§–∞–π–ª 'titanic.csv' –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ç–æ–π –∂–µ –ø–∞–ø–∫–µ, —á—Ç–æ –∏ —Å–∫—Ä–∏–ø—Ç")
    except Exception as e:
        print(f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
